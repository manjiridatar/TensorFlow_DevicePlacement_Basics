{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Device Placements in TensorFlow2 \n",
        "\n",
        "Make sure to select GPU runtime type (Runtime -> Change runtime type), before running this notebook\n",
        "\n",
        "In this notebook we will compare performance of execution on GPU vs CPU for matrix math and model compile and train"
      ],
      "metadata": {
        "id": "kpi2XTJpCYc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "93ki4NqHvSd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826d6af0-debd-4da0-f9a6-dc4fd188ed45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#List all physical devices. Lets write a function for this\n",
        "\n",
        "def fn_devicetypes(str_device_type=None):\n",
        "  list_device_types = tf.config.list_physical_devices(device_type=str_device_type)\n",
        "  return list_device_types"
      ],
      "metadata": {
        "id": "HHC0xU6Zvfu3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fn_devicetypes())\n",
        "print(fn_devicetypes(\"GPU\"))\n",
        "print(fn_devicetypes(\"CPU\"))"
      ],
      "metadata": {
        "id": "bzMRSj-w60qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f753e17-42d8-4721-fc7c-94b9dcb8375f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can get the GPU devicename using the tf.test.gpu_device_name()\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VYuas9XlD5Yf",
        "outputId": "96872bcb-d21b-4e1c-88ed-fdd19f0afc58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# TensorFlow automatically allocate Tensor operations to a physical device \n",
        "# and will handle the copying between CPU and GPU memory\n",
        "\n",
        "# Let's define a random Tensor and check the allocated device\n",
        "\n",
        "tensor_x = tf.random.uniform([3,3])\n",
        "tensor_x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-QKlLNB3ECOn",
        "outputId": "9508b405-520b-4a9c-9042-3534f31687e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The output string above will end with 'GPU:K' if the said Tensor is placed on the K-th GPU device.\n",
        "# We can also check if a tensor is placed on a specific devise by using the device_endswith which \n",
        "# returns either a True or a False\n",
        "\n",
        "print(tensor_x.device.endswith('CPU:0'))\n",
        "print(tensor_x.device.endswith('GPU:0'))\n",
        "print(tensor_x.device.endswith('GPU:1'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FdnCmEzEL8S",
        "outputId": "cc275896-2772-4762-f622-e074ef8ed5bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Matrix Addition and Multiplication Performance on GPU vs CPU"
      ],
      "metadata": {
        "id": "9iuLhqC0PQ49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can specify the device placement for a tensor if the device is available. \n",
        "# There are benefits to running tensor operations on the GPU and we will observe\n",
        "# this by performing matrix operations on the said tensor on the GPU and CPU\n",
        "# and compare them\n",
        "\n",
        "\n",
        "def matrix_addition(tensor_x, str_device_type):\n",
        "  with tf.device(str_device_type):\n",
        "    start = time.time()\n",
        "    for step in range(10):\n",
        "      tf.add(tensor_x, tensor_x)\n",
        "    time_elapsed = time.time() - start\n",
        "    print(\"Elapsed Time for Matrix Addition on the \" + str_device_type + \"{:0.2f}ms\".format(1000*time_elapsed))\n",
        "\n",
        "\n",
        "def matrix_multiply(tensor_x, str_device_type):\n",
        "  with tf.device(str_device_type):\n",
        "    start = time.time()\n",
        "    for step in range(10):\n",
        "      tf.multiply(tensor_x, tensor_x)\n",
        "    time_elapsed = time.time() - start\n",
        "    print(\"Elapsed Time for Matrix Multiplication on the \" + str_device_type + \"{:0.2f}ms\".format(1000*time_elapsed))\n"
      ],
      "metadata": {
        "id": "lnpeLcLJEZC0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creater a random tensor\n",
        "def init_tensor():\n",
        "  tensor_x = tf.random.uniform([3,3])\n",
        "  return tensor_x"
      ],
      "metadata": {
        "id": "tgQngUVDS-iL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "matrix_addition(init_tensor(), \"CPU:0\")\n",
        "matrix_addition(init_tensor(), \"GPU:0\")\n",
        "print(\"\\n\")\n",
        "matrix_multiply(init_tensor(), \"CPU:0\")\n",
        "matrix_multiply(init_tensor(), \"GPU:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dytL3z7iEtJw",
        "outputId": "53dd42bb-bbec-4a30-a76c-e7278c75be1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time for Matrix Addition on the CPU:01.19ms\n",
            "Elapsed Time for Matrix Addition on the GPU:00.67ms\n",
            "\n",
            "\n",
            "Elapsed Time for Matrix Multiplication on the CPU:05.66ms\n",
            "Elapsed Time for Matrix Multiplication on the GPU:00.51ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling and Training Performance on GPU vs CPU"
      ],
      "metadata": {
        "id": "lOPzaXO6PH7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This difference in time becomes more evident when we are actually training a model. \n",
        "# Lets try that out a simple image classification model using the MNIST database\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, y_train = x_train[:1000], y_train[:1000]\n",
        "x_train, x_test = x_train/255., x_test/255."
      ],
      "metadata": {
        "id": "l6pADq8iTh69"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets build the simple sequential model with a few Conv, maxpooling layers for the sake of this exercise\n",
        "\n",
        "# Let's define our get_model function\n",
        "\n",
        "def get_model():\n",
        "  model = Sequential([\n",
        "      layers.Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = (28,28,1)),\n",
        "      layers.MaxPool2D((2,2)),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation = 'relu'),\n",
        "      layers.Dense(10, activation = 'softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "rV9fWp7MPDrr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define our compile and fit function that takes the device name as a parameter\n",
        "\n",
        "def compile_fit_model(str_device_type):\n",
        "  with tf.device(str_device_type):\n",
        "    model = get_model()\n",
        "    model.compile(optimizer = RMSprop(1e-3), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "    start = time.time()\n",
        "    model.fit(x_train[...,np.newaxis], y_train, epochs = 6, verbose = 0)\n",
        "    elapsed_time = time.time() - start\n",
        "    print(\"Elapsed Time for compile and fit of the model on the \" + str_device_type + \"{:0.2f}ms\".format(1000*elapsed_time))\n"
      ],
      "metadata": {
        "id": "2sVXYBmBPiFb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call compile_fit_model for GPU device\n",
        "\n",
        "compile_fit_model(\"CPU:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc-WryKSP7cA",
        "outputId": "fc80ca15-3f50-4723-c335-20dbc13bfbc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time for compile and fit of the model on the CPU:08735.02ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call compile_fit_model for CPU device\n",
        "\n",
        "compile_fit_model(\"GPU:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsoUEcQQQGQu",
        "outputId": "97629a1b-bf0c-4a2b-dfff-2d822198a1e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time for compile and fit of the model on the GPU:05700.47ms\n"
          ]
        }
      ]
    }
  ]
}