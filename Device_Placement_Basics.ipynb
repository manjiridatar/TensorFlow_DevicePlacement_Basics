{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Device Placements in TensorFlow2 \n",
        "\n",
        "Make sure to select GPU runtime type (Runtime -> Change runtime type), before running this notebook\n",
        "\n",
        "In this notebook we will compare performance of execution on GPU vs CPU for matrix math and model compile and train"
      ],
      "metadata": {
        "id": "kpi2XTJpCYc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "93ki4NqHvSd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360f27d0-7d0b-42e1-dd91-9608ad5a719d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List physical devices\n",
        "\n",
        "tf.config.list_physical_devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DptOmEeoNZuu",
        "outputId": "37ed6a6a-8d49-499f-f3dd-e81c96849846"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List GPU devices\n",
        "\n",
        "tf.config.list_physical_devices(device_type=\"GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_JSAmxjUqW0",
        "outputId": "9d416046-b89b-42f7-d888-23ad232e3135"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List CPU devices \n",
        "\n",
        "tf.config.list_physical_devices(device_type=\"CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJkscs9fYZlt",
        "outputId": "26e81aca-d906-480a-f18e-5d35f705367c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#List all physical devices. Lets write a function for this\n",
        "\n",
        "def fn_devicetypes(str_device_type=None):\n",
        "  list_device_types = tf.config.list_physical_devices(device_type=str_device_type)\n",
        "  return list_device_types"
      ],
      "metadata": {
        "id": "HHC0xU6Zvfu3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fn_devicetypes())\n",
        "print(fn_devicetypes(\"GPU\"))\n",
        "print(fn_devicetypes(\"CPU\"))"
      ],
      "metadata": {
        "id": "bzMRSj-w60qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "205cfe2c-edc8-4cd2-a761-351cc6da767d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can get the GPU devicename using the tf.test.gpu_device_name()\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VYuas9XlD5Yf",
        "outputId": "c9170ad7-f510-4915-8afa-c2a415de4fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# TensorFlow automatically allocate Tensor operations to a physical device \n",
        "# and will handle the copying between CPU and GPU memory\n",
        "\n",
        "# Let's define a random Tensor and check the allocated device\n",
        "\n",
        "tensor_x = tf.random.uniform([3,3])\n",
        "tensor_x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-QKlLNB3ECOn",
        "outputId": "21384bf6-520a-4d59-b06b-f1995d422b9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/job:localhost/replica:0/task:0/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The output string above will end with 'GPU:K' if the said Tensor is placed on the K-th GPU device.\n",
        "# We can also check if a tensor is placed on a specific devise by using the device_endswith which \n",
        "# returns either a True or a False\n",
        "\n",
        "print(tensor_x.device.endswith('CPU:0'))\n",
        "print(tensor_x.device.endswith('GPU:0'))\n",
        "print(tensor_x.device.endswith('GPU:1'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FdnCmEzEL8S",
        "outputId": "3a2183bc-e1d4-4be6-bb47-1b2b9307b2a0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Matrix Addition and Multiplication Performance on GPU vs CPU"
      ],
      "metadata": {
        "id": "9iuLhqC0PQ49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can specify the device placement for a tensor if the device is available. \n",
        "# There are benefits to running tensor operations on the GPU and we will observe\n",
        "# this by performing matrix operations on the said tensor on the GPU and CPU\n",
        "# and compare them\n",
        "\n",
        "\n",
        "def matrix_addition(tensor_x, str_device_name):\n",
        "  with tf.device(str_device_name):\n",
        "    start = time.time()\n",
        "    for step in range(10):\n",
        "      tf.add(tensor_x, tensor_x)\n",
        "    time_elapsed = time.time() - start\n",
        "    print(\"Elapsed Time for Matrix Addition on the \" + str_device_name + \"{:0.2f}ms\".format(1000*time_elapsed))\n",
        "\n",
        "\n",
        "def matrix_multiply(tensor_x, str_device_name):\n",
        "  with tf.device(str_device_name):\n",
        "    start = time.time()\n",
        "    for step in range(10):\n",
        "      tf.multiply(tensor_x, tensor_x)\n",
        "    time_elapsed = time.time() - start\n",
        "    print(\"Elapsed Time for Matrix Multiplication on the \" + str_device_name + \"{:0.2f}ms\".format(1000*time_elapsed))\n"
      ],
      "metadata": {
        "id": "lnpeLcLJEZC0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creater a random tensor\n",
        "def init_tensor():\n",
        "  tensor_x = tf.random.uniform([3,3])\n",
        "  return tensor_x"
      ],
      "metadata": {
        "id": "tgQngUVDS-iL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "matrix_addition(init_tensor(), \"CPU:0\")\n",
        "matrix_addition(init_tensor(), \"GPU:0\")\n",
        "print(\"\\n\")\n",
        "matrix_multiply(init_tensor(), \"CPU:0\")\n",
        "matrix_multiply(init_tensor(), \"GPU:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dytL3z7iEtJw",
        "outputId": "7999f9b4-f8f8-4805-c70f-70e25ad71535"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time for Matrix Addition on the CPU:01.40ms\n",
            "Elapsed Time for Matrix Addition on the GPU:00.56ms\n",
            "\n",
            "\n",
            "Elapsed Time for Matrix Multiplication on the CPU:00.56ms\n",
            "Elapsed Time for Matrix Multiplication on the GPU:00.31ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling and Training Performance on GPU vs CPU"
      ],
      "metadata": {
        "id": "lOPzaXO6PH7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This difference in time becomes more evident when we are actually training a model. \n",
        "# Lets try that out a simple image classification model using the MNIST database\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, y_train = x_train[:1000], y_train[:1000]\n",
        "x_train, x_test = x_train/255., x_test/255."
      ],
      "metadata": {
        "id": "l6pADq8iTh69"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets build the simple sequential model with a few Conv, maxpooling layers for the sake of this exercise\n",
        "\n",
        "# Let's define our get_model function\n",
        "\n",
        "def get_model():\n",
        "  model = Sequential([\n",
        "      layers.Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = (28,28,1)),\n",
        "      layers.MaxPool2D((2,2)),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation = 'relu'),\n",
        "      layers.Dense(10, activation = 'softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "rV9fWp7MPDrr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define our compile and fit function that takes the device name as a parameter\n",
        "\n",
        "def compile_fit_model(str_device_name):\n",
        "  with tf.device(str_device_name):\n",
        "    model = get_model()\n",
        "    model.compile(optimizer = RMSprop(1e-3), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "    start = time.time()\n",
        "    model.fit(x_train[...,np.newaxis], y_train, epochs = 6, verbose = 0)\n",
        "    elapsed_time = time.time() - start\n",
        "    print(\"Elapsed Time for compile and fit of the model on the \" + str_device_name + \"{:0.2f}ms\".format(1000*elapsed_time))\n"
      ],
      "metadata": {
        "id": "2sVXYBmBPiFb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call compile_fit_model for CPU device\n",
        "\n",
        "compile_fit_model(\"CPU:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc-WryKSP7cA",
        "outputId": "a07440fe-3c35-47c2-d4c1-1e71f3ddea70"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time for compile and fit of the model on the CPU:08937.46ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call compile_fit_model for GPU device\n",
        "\n",
        "compile_fit_model(\"GPU:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsoUEcQQQGQu",
        "outputId": "46ced4f4-63c6-42cd-996f-f2bc6864e606"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time for compile and fit of the model on the GPU:01341.61ms\n"
          ]
        }
      ]
    }
  ]
}